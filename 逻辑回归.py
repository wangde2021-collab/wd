# ------------------------------------------------------------
# é¡¹ç›®ï¼šä¹³è…ºç™Œè‰¯æ¶æ€§äºŒåˆ†ç±»ï¼ˆé€»è¾‘å›å½’ï¼‰
# åŠŸèƒ½ï¼šæ•°æ®åŠ è½½ â†’ æ ‡å‡†åŒ– â†’ å»ºæ¨¡ â†’ å¤šç»´åº¦è¯„ä¼° â†’ ç‰¹å¾é‡è¦æ€§åˆ†æ
# ç¯å¢ƒè¦æ±‚ï¼šPython + sklearn + matplotlib + seaborn + pandas
# ------------------------------------------------------------

# ----------------------------
# 1. å¯¼å…¥æ‰€éœ€åº“
# ----------------------------
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# ----------------------------
# 2. è§£å†³ Matplotlib ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
# ----------------------------
# è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
plt.rcParams['font.sans-serif'] = [
    'SimHei',           # Windows é»‘ä½“
    'Microsoft YaHei',  # Windows å¾®è½¯é›…é»‘
    'PingFang SC',      # Mac è‹¹æ–¹
    'Arial Unicode MS', # è·¨å¹³å° Unicode å­—ä½“
    'DejaVu Sans'       # å¼€æºå¤‡ç”¨å­—ä½“
]
plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·ï¼ˆå¦‚ -0.5ï¼‰

# ----------------------------
# 3. åŠ è½½æ•°æ®é›†
# ----------------------------
data = load_breast_cancer()
X, y = data.data, data.target          # X: ç‰¹å¾çŸ©é˜µ (569Ã—30), y: æ ‡ç­¾ (0=æ¶æ€§, 1=è‰¯æ€§)
feature_names = data.feature_names     # ç‰¹å¾åç§°åˆ—è¡¨
target_names = data.target_names       # ['malignant', 'benign']

print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š{data.DESCR.split('..')[0].strip()}")  # æ‰“å°ç®€è¦æè¿°
print(f"æ ·æœ¬æ•°: {X.shape[0]}, ç‰¹å¾æ•°: {X.shape[1]}")
print(f"ç±»åˆ«åˆ†å¸ƒ: æ¶æ€§={sum(y==0)}, è‰¯æ€§={sum(y==1)}\n")

# ----------------------------
# 4. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼ˆ8:2ï¼Œåˆ†å±‚æŠ½æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # ç¡®ä¿è®­ç»ƒ/æµ‹è¯•é›†ä¸­æ¶æ€§:è‰¯æ€§æ¯”ä¾‹ä¸€è‡´
)

# ----------------------------
# 5. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆé˜²æ­¢é‡çº²å½±å“æ¨¡å‹ï¼‰
# æ³¨æ„ï¼šåªç”¨è®­ç»ƒé›†æ‹Ÿåˆ scalerï¼Œé¿å…æ•°æ®æ³„éœ²ï¼
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒé›†
X_test_scaled = scaler.transform(X_test)        # ä»…è½¬æ¢æµ‹è¯•é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰

# ----------------------------
# 6. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
# ----------------------------
model = LogisticRegression(
    max_iter=10000,   # å¢åŠ è¿­ä»£æ¬¡æ•°ç¡®ä¿æ”¶æ•›
    random_state=42   # ä¿è¯ç»“æœå¯å¤ç°
)
model.fit(X_train_scaled, y_train)

# ----------------------------
# 7. é¢„æµ‹
# ----------------------------
y_pred = model.predict(X_test_scaled)               # é¢„æµ‹ç±»åˆ«
y_pred_proba = model.predict_proba(X_test_scaled)   # é¢„æµ‹æ¦‚ç‡ï¼Œå½¢çŠ¶ (n, 2)
y_pred_proba_positive = y_pred_proba[:, 1]          # å–â€œè‰¯æ€§â€ï¼ˆæ­£ç±»ï¼‰çš„æ¦‚ç‡ç”¨äº ROC

# ----------------------------
# 8. æ¨¡å‹è¯„ä¼°
# ----------------------------
print("ğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°")
print("-" * 50)
print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC (ROCæ›²çº¿ä¸‹é¢ç§¯): {roc_auc_score(y_test, y_pred_proba_positive):.4f}")
print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=target_names))

# ----------------------------
# 9. å¯è§†åŒ–ï¼šæ··æ·†çŸ©é˜µ + ROC æ›²çº¿
# ----------------------------
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# --- æ··æ·†çŸ©é˜µ ---
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=target_names,
    yticklabels=target_names,
    ax=axes[0]
)
axes[0].set_title('æ··æ·†çŸ©é˜µ')
axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')

# --- ROC æ›²çº¿ ---
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_positive)
roc_auc = auc(fpr, tpr)
axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label='éšæœºåˆ†ç±»å™¨')
axes[1].set_xlim([0.0, 1.0])
axes[1].set_ylim([0.0, 1.05])
axes[1].set_xlabel('å‡æ­£ç‡ (False Positive Rate)')
axes[1].set_ylabel('çœŸæ­£ç‡ (True Positive Rate)')
axes[1].set_title('ROC æ›²çº¿')
axes[1].legend(loc="lower right")

plt.tight_layout()
plt.show()

# ----------------------------
# 10. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆé€»è¾‘å›å½’ç³»æ•°ï¼‰
# ----------------------------
coef = model.coef_[0]  # é€»è¾‘å›å½’å¯¹æ¯ä¸ªç‰¹å¾çš„æƒé‡ï¼ˆé•¿åº¦=30ï¼‰
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coef,
    'Abs_Coefficient': np.abs(coef)
}).sort_values(by='Abs_Coefficient', ascending=False)

print("\nğŸ” å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆæŒ‰æƒé‡ç»å¯¹å€¼æ’åºï¼‰:")
print(feature_importance_df[['Feature', 'Coefficient']].head(10).to_string(index=False))

# --- å¯è§†åŒ–å‰10é‡è¦ç‰¹å¾ ---
top_n = 10
top_features = feature_importance_df.head(top_n)

plt.figure(figsize=(10, 6))
colors = ['red' if c < 0 else 'green' for c in top_features['Coefficient']]
plt.barh(range(top_n), top_features['Coefficient'], color=colors)
plt.yticks(range(top_n), top_features['Feature'])
plt.xlabel('é€»è¾‘å›å½’ç³»æ•°')
plt.title(f'å‰ {top_n} ä¸ªæœ€é‡è¦ç‰¹å¾çš„æƒé‡\nï¼ˆçº¢è‰²ï¼šå¢å¤§è¯¥ç‰¹å¾ â†’ æ›´å¯èƒ½æ˜¯æ¶æ€§ï¼›ç»¿è‰²ï¼šæ›´å¯èƒ½æ˜¯è‰¯æ€§ï¼‰')
plt.gca().invert_yaxis()  # æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨
plt.tight_layout()
plt.show()