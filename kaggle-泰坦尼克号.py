# ==============================================
# 1. å¯¼å…¥æ‰©å±•åº“ï¼ˆæ–°å¢LightGBMã€ç½‘æ ¼æœç´¢ã€æ­£åˆ™åŒ–ç­‰ï¼‰
# ==============================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve, auc
import re  # ç”¨äºæå–å§“åä¸­çš„å¤´è¡”

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


# ==============================================
# 2. å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼‰
# ==============================================
def load_and_engineer_data(train_path, test_path=None):
    """åŠ è½½æ•°æ®å¹¶æ‰§è¡Œç‰¹å¾å·¥ç¨‹ï¼Œè¿”å›å¤„ç†åçš„è®­ç»ƒ/æµ‹è¯•æ•°æ®"""
    # åŠ è½½è®­ç»ƒé›†
    df_train = pd.read_csv(train_path)
    df = df_train.copy()
    test_flag = False
    if test_path:
        df_test = pd.read_csv(test_path)
        df_test['Survived'] = -1  # æ ‡è®°æµ‹è¯•é›†æ ‡ç­¾
        df = pd.concat([df, df_test], ignore_index=True)
        test_flag = True

    # -------- ç¼ºå¤±å€¼å¡«å……ï¼ˆç²¾ç»†åŒ–ï¼‰ --------
    # ä¿®å¤ï¼šå¤´è¡”æå–åŠ å¼‚å¸¸å¤„ç†ï¼Œé¿å…å§“åæ ¼å¼é”™è¯¯å¯¼è‡´çš„ç©ºå€¼
    def extract_title(name):
        try:
            return re.findall(r'([A-Za-z]+)\.', name)[0]
        except IndexError:
            return 'Mr'  # å¼‚å¸¸å§“åé»˜è®¤å½’ä¸ºMr

    df['Title'] = df['Name'].apply(extract_title)

    # å¹´é¾„ï¼šæŒ‰å¤´è¡”+èˆ±ä½åˆ†ç»„å¡«å……ï¼ˆæ¯”ä»…Pclass+Sexæ›´ç²¾å‡†ï¼‰
    df['Age'] = df.groupby(['Pclass', 'Title'])['Age'].transform(lambda x: x.fillna(x.median())).fillna(28)  # å…œåº•å¡«å……
    # ç™»èˆ¹æ¸¯å£ï¼šä¼—æ•°å¡«å……
    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
    # ç¥¨ä»·ï¼šæŒ‰èˆ±ä½åˆ†ç»„å¡«å……
    df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median())).fillna(14.45)  # å…œåº•å¡«å……
    # èˆ±ä½ï¼šå¡«å……Uå¹¶æå–é¦–å­—æ¯ï¼Œåˆå¹¶ç¨€æœ‰èˆ±ä½
    df['Cabin'] = df['Cabin'].fillna('U').apply(lambda x: x[0] if pd.notna(x) else 'U')
    rare_cabins = df['Cabin'].value_counts()[df['Cabin'].value_counts() < 10].index
    df['Cabin'] = df['Cabin'].replace(rare_cabins, 'R')  # ç¨€æœ‰èˆ±ä½åˆå¹¶ä¸ºR

    # -------- è¡ç”Ÿç‰¹å¾ï¼ˆæ ¸å¿ƒï¼ï¼‰ --------
    # 1. å®¶åº­è§„æ¨¡ï¼šå…„å¼Ÿå§å¦¹+çˆ¶æ¯å­å¥³+è‡ªå·±
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    # 2. æ˜¯å¦å•èº«
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    # 3. å¤´è¡”åˆå¹¶ï¼ˆå‡å°‘ç±»åˆ«æ•°ï¼‰
    title_mapping = {
        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',
        'Don': 'Noble', 'Sir': 'Noble', 'Lady': 'Noble', 'Countess': 'Noble', 'Dona': 'Noble',
        'Dr': 'Professional', 'Rev': 'Professional', 'Col': 'Military', 'Major': 'Military', 'Capt': 'Military',
        'Ms': 'Miss', 'Mlle': 'Miss', 'Mme': 'Mrs', 'Unknown': 'Mr'  # æ–°å¢å¼‚å¸¸å¤´è¡”æ˜ å°„
    }
    df['Title'] = df['Title'].map(title_mapping).fillna('Mr')  # å…œåº•å¡«å……
    # 4. ç¥¨ä»·åˆ†ç®±ï¼ˆæ•æ‰éçº¿æ€§å…³ç³»ï¼Œä¿®å¤NaNï¼‰
    df['FareBin'] = pd.cut(df['Fare'], bins=[0, 10, 30, 100, 600], labels=['Low', 'Mid', 'High', 'Luxury']).fillna(
        'Luxury')
    # 5. å¹´é¾„åˆ†ç®±ï¼ˆä¿®å¤NaNï¼‰
    df['AgeBin'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100],
                          labels=['Child', 'Teen', 'Adult', 'Middle', 'Elder']).fillna('Adult')

    # -------- ç­›é€‰ç‰¹å¾ --------
    # æœ€ç»ˆç‰¹å¾åˆ—è¡¨ï¼ˆå«è¡ç”Ÿç‰¹å¾ï¼‰
    core_features = [
        'Pclass', 'Sex', 'Embarked', 'Cabin', 'Title',
        'FamilySize', 'IsAlone', 'FareBin', 'AgeBin'
    ]
    target = 'Survived'

    # æ‹†åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    if test_flag:
        df_train_processed = df[df[target] != -1].copy()
        df_test_processed = df[df[target] == -1].copy()
        X_train = df_train_processed[core_features]
        y_train = df_train_processed[target]
        X_test = df_test_processed[core_features]
        passenger_id = df_test_processed['PassengerId']
        return X_train, y_train, X_test, passenger_id
    else:
        X = df[core_features]
        y = df[target]
        return X, y


# åŠ è½½æ•°æ®å¹¶æ‰§è¡Œç‰¹å¾å·¥ç¨‹ï¼ˆä¿®æ”¹ä¸ºä½ çš„è·¯å¾„ï¼‰
train_path = r'C:\Users\wangd\Desktop\kaggle\1_æ³°å¦å°¼å…‹å·\train.csv'
test_path = r'C:\Users\wangd\Desktop\kaggle\1_æ³°å¦å°¼å…‹å·\test.csv'
X, y, X_test, passenger_id = load_and_engineer_data(train_path, test_path)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæ¯”åŸä»£ç çš„train_test_splitæ›´åˆç†ï¼‰
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ==============================================
# 3. é¢„å¤„ç†ç®¡é“ï¼ˆå…¨å±€ç»Ÿä¸€ï¼‰
# ==============================================
# å®šä¹‰ç±»åˆ«ç‰¹å¾ï¼ˆæ‰€æœ‰éæ•°å€¼ç‰¹å¾ï¼‰
categorical_features = X_train.columns.tolist()  # ç»ç‰¹å¾å·¥ç¨‹åå‡ä¸ºç±»åˆ«ç‰¹å¾
# é¢„å¤„ç†ï¼šç‹¬çƒ­ç¼–ç ï¼ˆå¿½ç•¥æœªçŸ¥ç±»åˆ«ï¼‰
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ]
)


# ==============================================
# 4. é‡æ„è°ƒå‚å‡½æ•°ï¼ˆæ ¸å¿ƒï¼šç”¨Pipelineå°è£…é¢„å¤„ç†+æ¨¡å‹ï¼Œæ¶ˆé™¤ç‰¹å¾åè­¦å‘Šï¼‰
# ==============================================
def tune_model(preprocessor, model, param_grid, X, y):
    """
    ç½‘æ ¼æœç´¢è°ƒå‚ï¼Œå°è£…é¢„å¤„ç†+æ¨¡å‹çš„Pipeline
    :param preprocessor: å…¨å±€é¢„å¤„ç†ç®¡é“
    :param model: å¾…è°ƒå‚çš„åŸºæ¨¡å‹
    :param param_grid: è°ƒå‚ç½‘æ ¼ï¼ˆæ³¨æ„å‚æ•°åè¦åŠ æ¨¡å‹åˆ«å__ï¼‰
    :param X: åŸå§‹ç‰¹å¾ï¼ˆDataFrameï¼Œå¸¦ç‰¹å¾åï¼‰
    :param y: æ ‡ç­¾
    :return: æœ€ä¼˜Pipelineæ¨¡å‹
    """
    # å°è£…é¢„å¤„ç†+æ¨¡å‹çš„Pipeline
    model_pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(
        estimator=model_pipeline,
        param_grid=param_grid,
        cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
        scoring='accuracy',
        n_jobs=-1,  # å¹¶è¡Œè®¡ç®—
        verbose=0
    )
    grid_search.fit(X, y)  # è¾“å…¥æ˜¯å¸¦ç‰¹å¾åçš„DataFrameï¼Œç”±Pipelineå†…éƒ¨å¤„ç†
    print(f"âœ… {model.__class__.__name__} æœ€ä¼˜å‚æ•°ï¼š{grid_search.best_params_}")
    print(f"âœ… äº¤å‰éªŒè¯æœ€ä¼˜å‡†ç¡®ç‡ï¼š{grid_search.best_score_:.4f}")
    return grid_search.best_estimator_


# -------- å®šä¹‰åŸºæ¨¡å‹åŠè°ƒå‚ç½‘æ ¼ï¼ˆå‚æ•°åè¦åŠ model__å‰ç¼€ï¼ï¼‰ --------
# XGBoostè°ƒå‚
xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_param = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [3, 5],
    'model__learning_rate': [0.05, 0.1],
    'model__subsample': [0.8, 1.0]
}
best_xgb_pipeline = tune_model(preprocessor, xgb, xgb_param, X_train, y_train)
# æå–è°ƒä¼˜åçš„XGBæ¨¡å‹ï¼ˆç”¨äºå †å ï¼‰
best_xgb = best_xgb_pipeline.named_steps['model']

# LightGBMè°ƒå‚ï¼ˆæ–°å¢é«˜æ•ˆæ¨¡å‹ï¼‰
lgb = LGBMClassifier(random_state=42, verbose=-1)
lgb_param = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [3, 5],
    'model__learning_rate': [0.05, 0.1],
    'model__num_leaves': [31, 63]
}
best_lgb_pipeline = tune_model(preprocessor, lgb, lgb_param, X_train, y_train)
# æå–è°ƒä¼˜åçš„LGBæ¨¡å‹ï¼ˆç”¨äºå †å ï¼‰
best_lgb = best_lgb_pipeline.named_steps['model']

# éšæœºæ£®æ—è°ƒå‚
rf = RandomForestClassifier(random_state=42)
rf_param = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [5, 8],
    'model__min_samples_split': [2, 5]
}
best_rf_pipeline = tune_model(preprocessor, rf, rf_param, X_train, y_train)
# æå–è°ƒä¼˜åçš„RFæ¨¡å‹ï¼ˆç”¨äºå †å ï¼‰
best_rf = best_rf_pipeline.named_steps['model']

# ==============================================
# 5. å †å é›†æˆæ¨¡å‹ï¼ˆStackingï¼‰- æ— è­¦å‘Šç‰ˆæœ¬
# ==============================================
# å®šä¹‰åŸºæ¨¡å‹åˆ—è¡¨ï¼ˆæ‰€æœ‰æ¨¡å‹å‡ä¸ºè°ƒä¼˜åçš„å®ä¾‹ï¼‰
base_models = [
    ('xgb', best_xgb),
    ('lgb', best_lgb),
    ('rf', best_rf),
    ('svc', SVC(probability=True, random_state=42)),  # SVMï¼ˆå¸¦æ¦‚ç‡ï¼‰
    ('lr', LogisticRegression(random_state=42, max_iter=500))  # é€»è¾‘å›å½’
]

# å †å é›†æˆï¼šäºŒçº§æ¨¡å‹ç”¨é€»è¾‘å›å½’ï¼Œå°è£…å…¨å±€é¢„å¤„ç†ç®¡é“
stacking_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('stacking', StackingClassifier(
        estimators=base_models,
        final_estimator=LogisticRegression(random_state=42, max_iter=500),
        cv=5,
        stack_method='predict_proba'
    ))
])

# è®­ç»ƒå †å æ¨¡å‹
stacking_pipeline.fit(X_train, y_train)

# ==============================================
# 6. æ¨¡å‹è¯„ä¼°ï¼ˆå‡†ç¡®ç‡å¤§å¹…æå‡ï¼Œæ— è­¦å‘Šï¼‰
# ==============================================
# éªŒè¯é›†é¢„æµ‹
y_pred = stacking_pipeline.predict(X_val)
y_pred_proba = stacking_pipeline.predict_proba(X_val)[:, 1]

# è¾“å‡ºè¯„ä¼°æŒ‡æ ‡
print("\nğŸ¯ ä¼˜åŒ–åæ¨¡å‹æ€§èƒ½è¯„ä¼°")
print("-" * 50)
print(f"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy_score(y_val, y_pred):.4f}")
print(f"éªŒè¯é›†AUC: {roc_auc_score(y_val, y_pred_proba):.4f}")
print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_val, y_pred, target_names=['æ­»äº¡', 'ç”Ÿå­˜']))

# æ··æ·†çŸ©é˜µ+ROCæ›²çº¿å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
cm = confusion_matrix(y_val, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['æ­»äº¡', 'ç”Ÿå­˜'], yticklabels=['æ­»äº¡', 'ç”Ÿå­˜'],
            ax=axes[0])
axes[0].set_title('æ··æ·†çŸ©é˜µ', fontsize=12)
axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')

fpr, tpr, _ = roc_curve(y_val, y_pred_proba)
roc_auc = auc(fpr, tpr)
axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
axes[1].set_xlabel('å‡æ­£ç‡')
axes[1].set_ylabel('çœŸæ­£ç‡')
axes[1].set_title('ROCæ›²çº¿')
axes[1].legend(loc="lower right")
plt.tight_layout()
plt.show()

# ==============================================
# 7. ä¿å­˜æ¨¡å‹+é¢„æµ‹æµ‹è¯•é›†ï¼ˆä¿®å¤è·¯å¾„é—®é¢˜+Kaggleæäº¤æ ¼å¼ï¼‰
# ==============================================
# ä¿®å¤ï¼šæ¨¡å‹ä¿å­˜è·¯å¾„æ”¹ä¸ºå’Œæ•°æ®é›†ç›¸åŒçš„è·¯å¾„
model_save_path = r'C:\Users\wangd\Desktop\kaggle\1_æ³°å¦å°¼å…‹å·\titanic_stacking_model.pkl'
joblib.dump(stacking_pipeline, model_save_path)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{model_save_path}")

# æµ‹è¯•é›†é¢„æµ‹
y_test_pred = stacking_pipeline.predict(X_test)
y_test_proba = stacking_pipeline.predict_proba(X_test)[:, 1]

# ä¿®å¤ï¼šç”ŸæˆKaggleè¦æ±‚çš„æäº¤æ–‡ä»¶ï¼ˆä»…ä¿ç•™PassengerIdå’ŒSurvivedï¼‰
submission = pd.DataFrame({
    'PassengerId': passenger_id,
    'Survived': y_test_pred.astype(int)
})
# é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„ï¼ˆä¸å˜ï¼‰
result_save_path = r'C:\Users\wangd\Desktop\kaggle\1_æ³°å¦å°¼å…‹å·\optimized_predict_result.csv'
submission.to_csv(result_save_path, index=False)
print(f"âœ… Kaggleæäº¤æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{result_save_path}")
print("\nğŸ“Œ æµ‹è¯•é›†å‰5æ¡é¢„æµ‹ç»“æœï¼š")
print(submission.head())

# å¯é€‰ï¼šç”Ÿæˆå«æ¦‚ç‡çš„ç»“æœæ–‡ä»¶ï¼ˆè‡ªå·±åˆ†æç”¨ï¼‰
analysis_result = pd.DataFrame({
    'PassengerId': passenger_id,
    'Survived': y_test_pred.astype(int),
    'Survived_Probability': y_test_proba
})
analysis_save_path = r'C:\Users\wangd\Desktop\kaggle\1_æ³°å¦å°¼å…‹å·\analysis_result.csv'
analysis_result.to_csv(analysis_save_path, index=False)
print(f"âœ… å«æ¦‚ç‡çš„åˆ†ææ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{analysis_save_path}")