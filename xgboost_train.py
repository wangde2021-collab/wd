# ------------------------------------------------------------
# é¡¹ç›®ï¼šä¹³è…ºç™Œè‰¯æ¶æ€§äºŒåˆ†ç±»ï¼ˆXGBoost å®ç°ï¼‰
# åŠŸèƒ½ï¼šæ•°æ®åŠ è½½ â†’ æ ‡å‡†åŒ– â†’ XGBoostå»ºæ¨¡ â†’ è°ƒå‚ â†’ è¯„ä¼° â†’ å¯è§†åŒ–
# é€‚ç”¨ï¼šUpwork / Kaggle / æ•™å­¦ / åŒ»ç–—è¾…åŠ©è¯Šæ–­åŸå‹
# ------------------------------------------------------------

# ----------------------------
# 1. å¯¼å…¥åº“
# ----------------------------
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc, roc_auc_score
)
import xgboost_train as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy.stats import randint, uniform

# ----------------------------
# 2. è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚ä¸éœ€è¦ï¼Œå¯åˆ é™¤ï¼‰
# ----------------------------
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ----------------------------
# 3. åŠ è½½æ•°æ®
# ----------------------------
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

print(f"ğŸ“Š æ•°æ®é›†ï¼š{data.DESCR.split('..')[0].strip()}")
print(f"æ ·æœ¬æ•°: {X.shape[0]}, ç‰¹å¾æ•°: {X.shape[1]}")
print(f"ç±»åˆ«åˆ†å¸ƒ: æ¶æ€§={np.sum(y==0)}, è‰¯æ€§={np.sum(y==1)}\n")

# ----------------------------
# 4. åˆ’åˆ†æ•°æ®é›†
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 5. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆXGBoost ä¸å¼ºåˆ¶éœ€è¦ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æµç¨‹ï¼‰
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 6. è¶…å‚æ•°è°ƒä¼˜ï¼ˆè½»é‡çº§éšæœºæœç´¢ï¼‰
# ----------------------------
param_dist = {
    'n_estimators': randint(50, 300),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),  # 0.01 ~ 0.31
    'subsample': uniform(0.6, 0.4),       # 0.6 ~ 1.0
    'colsample_bytree': uniform(0.6, 0.4)
}

xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)

# éšæœºæœç´¢ï¼ˆåªè¯• 30 ç»„ï¼Œå¿«é€Ÿé«˜æ•ˆï¼‰
random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=30,
    scoring='roc_auc',
    cv=3,  # 3æŠ˜åŠ é€Ÿ
    random_state=42,
    n_jobs=-1,
    verbose=0
)

print("ğŸ” æ­£åœ¨è¿›è¡Œ XGBoost è¶…å‚æ•°è°ƒä¼˜ï¼ˆçº¦éœ€ 10~30 ç§’ï¼‰...")
random_search.fit(X_train_scaled, y_train)

best_model = random_search.best_estimator_
print(f"âœ… æœ€ä¼˜å‚æ•°: {random_search.best_params_}")
print(f"âœ… äº¤å‰éªŒè¯ AUC: {random_search.best_score_:.4f}\n")

# ----------------------------
# 7. é¢„æµ‹
# ----------------------------
y_pred = best_model.predict(X_test_scaled)
y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # è‰¯æ€§æ¦‚ç‡

# ----------------------------
# 8. è¯„ä¼°
# ----------------------------
print("ğŸ¯ XGBoost æ¨¡å‹æ€§èƒ½")
print("-" * 40)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"æµ‹è¯•é›† AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=target_names))

# ----------------------------
# 9. å¯è§†åŒ–
# ----------------------------
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names, ax=axes[0])
axes[0].set_title('æ··æ·†çŸ©é˜µ')
axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')

# ROC æ›²çº¿
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
axes[1].set_xlim([0.0, 1.0])
axes[1].set_ylim([0.0, 1.05])
axes[1].set_xlabel('å‡æ­£ç‡')
axes[1].set_ylabel('çœŸæ­£ç‡')
axes[1].set_title('ROC æ›²çº¿')
axes[1].legend(loc="lower right")

plt.tight_layout()
plt.show()

# ----------------------------
# 10. ç‰¹å¾é‡è¦æ€§ï¼ˆXGBoost åŸç”Ÿæ”¯æŒï¼‰
# ----------------------------
importances = best_model.feature_importances_
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print("\nğŸ” å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
print(feat_imp_df.head(10).to_string(index=False))

# å¯è§†åŒ–
top_n = 10
top_feat = feat_imp_df.head(top_n)
plt.figure(figsize=(10, 6))
plt.barh(range(top_n), top_feat['Importance'], color='steelblue')
plt.yticks(range(top_n), top_feat['Feature'])
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.title(f'XGBoostï¼šå‰ {top_n} ä¸ªæœ€é‡è¦ç‰¹å¾')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()