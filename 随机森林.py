# ------------------------------------------------------------
# é¡¹ç›®ï¼šä¹³è…ºç™Œè‰¯æ¶æ€§äºŒåˆ†ç±»ï¼ˆéšæœºæ£®æ—ï¼‰
# åŠŸèƒ½ï¼šæ•°æ®åŠ è½½ â†’ æ ‡å‡†åŒ– â†’ éšæœºæ£®æ—å»ºæ¨¡ â†’ å¤šç»´åº¦è¯„ä¼° â†’ ç‰¹å¾é‡è¦æ€§åˆ†æ
# ä½œè€…ï¼šAIåŠ©æ‰‹
# ç¯å¢ƒè¦æ±‚ï¼šPython + sklearn + matplotlib + seaborn + pandas
# ------------------------------------------------------------

# ----------------------------
# 1. å¯¼å…¥æ‰€éœ€åº“
# ----------------------------
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# ----------------------------
# 2. è§£å†³ Matplotlib ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
# ----------------------------
plt.rcParams['font.sans-serif'] = [
    'SimHei', 'Microsoft YaHei', 'PingFang SC', 'Arial Unicode MS', 'DejaVu Sans'
]
plt.rcParams['axes.unicode_minus'] = False

# ----------------------------
# 3. åŠ è½½æ•°æ®é›†
# ----------------------------
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š{data.DESCR.split('..')[0].strip()}")
print(f"æ ·æœ¬æ•°: {X.shape[0]}, ç‰¹å¾æ•°: {X.shape[1]}")
print(f"ç±»åˆ«åˆ†å¸ƒ: æ¶æ€§={sum(y==0)}, è‰¯æ€§={sum(y==1)}\n")

# ----------------------------
# 4. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# ----------------------------
# 5. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯¹æ ‘æ¨¡å‹éå¿…éœ€ï¼Œä½†ä¿ç•™ä»¥ä¿æŒæµç¨‹ä¸€è‡´ï¼‰
# æ³¨æ„ï¼šéšæœºæ£®æ—å¯¹é‡çº²ä¸æ•æ„Ÿï¼Œä½†æ ‡å‡†åŒ–ä¸å½±å“ç»“æœ
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 6. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
# ----------------------------
model = RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=5,           # æ§åˆ¶è¿‡æ‹Ÿåˆï¼ˆå¯è°ƒï¼‰
    random_state=42,
    n_jobs=-1              # å¹¶è¡ŒåŠ é€Ÿ
)
model.fit(X_train_scaled, y_train)

# ----------------------------
# 7. é¢„æµ‹
# ----------------------------
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)
y_pred_proba_positive = y_pred_proba[:, 1]  # è‰¯æ€§ï¼ˆæ­£ç±»ï¼‰æ¦‚ç‡

# ----------------------------
# 8. æ¨¡å‹è¯„ä¼°
# ----------------------------
print("ğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆéšæœºæ£®æ—ï¼‰")
print("-" * 50)
print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC (ROCæ›²çº¿ä¸‹é¢ç§¯): {roc_auc_score(y_test, y_pred_proba_positive):.4f}")

# äº¤å‰éªŒè¯ï¼ˆæ›´ç¨³å¥çš„è¯„ä¼°ï¼‰
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
print(f"5æŠ˜äº¤å‰éªŒè¯ AUC å‡å€¼: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=target_names))

# ----------------------------
# 9. å¯è§†åŒ–ï¼šæ··æ·†çŸ©é˜µ + ROC æ›²çº¿
# ----------------------------
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names, ax=axes[0])
axes[0].set_title('æ··æ·†çŸ©é˜µ')
axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')

# ROC æ›²çº¿
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_positive)
roc_auc = auc(fpr, tpr)
axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label='éšæœºåˆ†ç±»å™¨')
axes[1].set_xlim([0.0, 1.0])
axes[1].set_ylim([0.0, 1.05])
axes[1].set_xlabel('å‡æ­£ç‡ (False Positive Rate)')
axes[1].set_ylabel('çœŸæ­£ç‡ (True Positive Rate)')
axes[1].set_title('ROC æ›²çº¿')
axes[1].legend(loc="lower right")

plt.tight_layout()
plt.show()

# ----------------------------
# 10. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆéšæœºæ£®æ—åŸç”Ÿæ”¯æŒï¼ï¼‰
# ----------------------------
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("\nğŸ” å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆéšæœºæ£®æ—ï¼‰:")
print(feature_importance_df.head(10).to_string(index=False))

# å¯è§†åŒ–å‰10é‡è¦ç‰¹å¾
top_n = 10
top_features = feature_importance_df.head(top_n)

plt.figure(figsize=(10, 6))
plt.barh(range(top_n), top_features['Importance'], color='steelblue')
plt.yticks(range(top_n), top_features['Feature'])
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.title(f'éšæœºæ£®æ—ï¼šå‰ {top_n} ä¸ªæœ€é‡è¦ç‰¹å¾')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()